---
title: "STAT 5361 - Homework 2"
author: "Jin Xu"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  pdf_document: default
documentclass: article
fontsize: 12pt
bibliography: template.bib
papersize: letter
biblio-style: datalab
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
## some utility functions, see the source code for details
source("utils_template.R")

## specify the packages needed
pkgs <- c("splines2", "DT", "webshot", "leaflet", "graphics")
need.packages(pkgs)

## external data can be read in by regular functions,
## such as read.table or load

## get output format in case something needs extra effort
outFormat <- knitr::opts_knit$get("rmarkdown.pandoc.to")
## "latex" or "html"

## for latex and html output
isHtml <- identical(outFormat, "html")
isLatex <- identical(outFormat, "latex")
latex <- ifelse(isLatex, '\\LaTeX\\', 'LaTeX')

## specify global chunk options
knitr::opts_chunk$set(fig.width = 5, fig.height = 4, dpi = 300,
                      out.width = "90%", fig.align = "center")

```


### Problem 1 {-}
#### Problem 1(a) {-}
log-likelihood  function: $l(\theta) = \ln({\sum_{i=1}^{n}\dfrac{1}{\pi[1 + (x - \theta)^2]}}) = -n\ln{\pi} - \sum_{i=1}^{n} \ln{[1 + (x - \theta)^2]}$

1st derivative of log-likelihood  function: $l'(\theta) = -2 \sum_{i=1}^{n} \dfrac{\theta - x_i}{1 + (x - \theta)^2}$

2nd derivative of log-likelihood  function: $l''(\theta) = -2 \sum_{i=1}^{n} \dfrac{1 - (\theta - x_i)^2}{[1 + (x - \theta)^2]^2}$

fisher information: $p'(x) =  \dfrac{2\pi(\theta - x)}{(\pi + \pi(x - \theta)^2)^2}$

$\dfrac{\{p'(x)\}^2}{p(x)} = 2(\theta-x)/{(1 + (x - \theta)^2)}$

$I(\theta) = n \int \dfrac{\{p'(x)\}^2}{p(x)}dx = \dfrac{4n}{\pi} \int_{-\infty}^{\infty} \dfrac{x^2 dx}{(1 + x^2)^3} = \dfrac{n}{2}$

#### Problem 1(b) {-}
The graph of the log-likelihood function of $\theta$ is plotted from $[-50,50]$.
```{r q1b1, echo = FALSE, eval=TRUE,fig.cap="\\label{fig.q1b} Log-likelihood function"}
# Plot the log-likelihood function
LogC <- function(theta,x) sum(-log(pi) - log(1 + (theta - x)^2))
x_sample <- c (1.77, -0.23, 2.76, 3.80, 3.47, 56.75, -1.34, 4.24, -2.44,
               3.29, 3.71, -2.40, 4.53, -0.07, -1.05, -13.87, -2.53, -1.75)
theta <- seq(-50, 50, 1)
plot (theta, sapply(theta, LogC, x_sample), type = "l", ylab = "LogL of Cauchy")
```

Find the MLE for $\theta$ using the Newton-Raphson method
```{r q1b2, echo = FALSE, eval=TRUE}
source('Newton-Raphson iterations_XJ.R')
x <- c (1.77, -0.23, 2.76, 3.80, 3.47, 56.75, -1.34, 4.24, -2.44,
               3.29, 3.71, -2.40, 4.53, -0.07, -1.05, -13.87, -2.53, -1.75)
gprime <- function(theta) sum((-2) * (theta - x) / (1 + (theta-x)^2))
g2prime <- function(theta) sum((2 * (x-theta)^2 - 2)/(1+(x-theta)^2)^2)

theta0 <- -11
result1 <- Newton_Raphson(theta0, gprime, g2prime)
cat("when the starting point is -11, the MLE for theta is" , result1)

theta0 <- -1
result2 <- Newton_Raphson(theta0, gprime, g2prime)
cat("when the starting point is -1, the MLE for theta is" , result2)

theta0 <- 0
result3 <- Newton_Raphson(theta0, gprime, g2prime)
cat("when the starting point is 0, the MLE for theta is" , result3)

theta0 <- 1.5
result4 <- Newton_Raphson(theta0, gprime, g2prime)
cat("when the starting point is -1.5, the MLE for theta is" , result4)

theta0 <- 4
result5 <- Newton_Raphson(theta0, gprime, g2prime)
cat("when the starting point is 4, the MLE for theta is" , result5)

theta0 <- 4.7
result6 <- Newton_Raphson(theta0, gprime, g2prime)
cat("when the starting point is 4.7, the MLE for theta is" , result6)

theta0 <- 7
result7 <- Newton_Raphson(theta0, gprime, g2prime)
cat("when the starting point is 7, the MLE for theta is" , result7)

theta0 <- 8
result8 <- Newton_Raphson(theta0, gprime, g2prime)
cat("when the starting point is 8, the MLE for theta is" , result8)

theta0 <- 38
result9 <- Newton_Raphson(theta0, gprime, g2prime)
cat("when the starting point is 38, the MLE for theta is" , result9)

theta0 <- mean(x)
result10 <- Newton_Raphson(theta0, gprime, g2prime)
cat("when the starting point is ",mean(x), "the MLE for theta is " , result10, ".   The sample mean is a good starting point.")
```

#### Problem 1(c) {-}
Apply fixed-point iterations to find the MLE
```{r q1c, echo = FALSE, eval=TRUE}
source('fixed_point_iterations_XJ.R')
x <- c (1.77, -0.23, 2.76, 3.80, 3.47, 56.75, -1.34, 4.24, -2.44,
               3.29, 3.71, -2.40, 4.53, -0.07, -1.05, -13.87, -2.53, -1.75)
gprime <- function(theta) sum((-2) * (theta - x) / (1 + (theta-x)^2))

alpha <- 1
theta0 <- -11
result11 <- fixedPoint(gprime, theta0, alpha)
cat("when the starting point is -11, alpha is 1, the MLE for theta is" , result11)

alpha <- 1
theta0 <- -1
result12 <- fixedPoint(gprime, theta0, alpha)
cat("when the starting point is -1, alpha is 1, the MLE for theta is" , result12)

alpha <- 1
theta0 <- 0
result13 <- fixedPoint(gprime, theta0, alpha)
cat("when the starting point is 0, alpha is 1, the MLE for theta is" , result13)

alpha <- 1
theta0 <- 1.5
result14 <- fixedPoint(gprime, theta0, alpha)
cat("when the starting point is 1.5, alpha is 1, the MLE for theta is" , result14)

alpha <- 1
theta0 <- 4
result15 <- fixedPoint(gprime, theta0, alpha)
cat("when the starting point is 4, alpha is 1, the MLE for theta is" , result15)

alpha <- 1
theta0 <- 4.7
result16 <- fixedPoint(gprime, theta0, alpha)
cat("when the starting point is 4.7, alpha is 1, the MLE for theta is" , result16)

alpha <- 1
theta0 <- 7
result17 <- fixedPoint(gprime, theta0, alpha)
cat("when the starting point is 7, alpha is 1, the MLE for theta is" , result17)

alpha <- 1
theta0 <- 8
result18 <- fixedPoint(gprime, theta0, alpha)
cat("when the starting point is 8, alpha is 1, the MLE for theta is" , result18)

alpha <- 1
theta0 <- 38
result19 <- fixedPoint(gprime, theta0, alpha)
cat("when the starting point is 38, alpha is 1, the MLE for theta is" , result19)

alpha <- 0.64
theta0 <- -11
result20 <- fixedPoint(gprime, theta0, alpha)
cat("when the starting point is -11, alpha is 0.64, the MLE for theta is" , result20)

alpha <- 0.64
theta0 <- -1
result21 <- fixedPoint(gprime, theta0, alpha)
cat("when the starting point is -1, alpha is 0.64, the MLE for theta is" , result21)

alpha <- 0.64
theta0 <- 0
result22 <- fixedPoint(gprime, theta0, alpha)
cat("when the starting point is 0, alpha is 0.64, the MLE for theta is" , result22)

alpha <- 0.64
theta0 <- 1.5
result23 <- fixedPoint(gprime, theta0, alpha)
cat("when the starting point is 1.5, alpha is 0.64, the MLE for theta is" , result23)

alpha <- 0.64
theta0 <- 4
result24 <- fixedPoint(gprime, theta0, alpha)
cat("when the starting point is 4, alpha is 0.64, the MLE for theta is" , result24)

alpha <- 0.64
theta0 <- 4.7
result25 <- fixedPoint(gprime, theta0, alpha)
cat("when the starting point is 4.7, alpha is 0.64, the MLE for theta is" , result25)

alpha <- 0.64
theta0 <- 7
result26 <- fixedPoint(gprime, theta0, alpha)
cat("when the starting point is 7, alpha is 0.64, the MLE for theta is" , result26)

alpha <- 0.64
theta0 <- 8
result27 <- fixedPoint(gprime, theta0, alpha)
cat("when the starting point is 8, alpha is 0.64, the MLE for theta is" , result27)

alpha <- 0.64
theta0 <- 38
result28 <- fixedPoint(gprime, theta0, alpha)
cat("when the starting point is 38, alpha is 0.64, the MLE for theta is" , result28)

alpha <- 0.25
theta0 <- -11
result29 <- fixedPoint(gprime, theta0, alpha)
cat("when the starting point is -11, alpha is 0.25, the MLE for theta is" , result29)

alpha <- 0.25
theta0 <- -1
result30 <- fixedPoint(gprime, theta0, alpha)
cat("when the starting point is -1, alpha is 0.25, the MLE for theta is" , result30)

alpha <- 0.25
theta0 <- 0
result31 <- fixedPoint(gprime, theta0, alpha)
cat("when the starting point is 0, alpha is 0.25, the MLE for theta is" , result31)

alpha <- 0.25
theta0 <- 1.5
result32 <- fixedPoint(gprime, theta0, alpha)
cat("when the starting point is 1.5, alpha is 0.25, the MLE for theta is" , result32)

alpha <- 0.25
theta0 <- 4
result33 <- fixedPoint(gprime, theta0, alpha)
cat("when the starting point is 4, alpha is 0.25, the MLE for theta is" , result33)

alpha <- 0.25
theta0 <- 4.7
result34 <- fixedPoint(gprime, theta0, alpha)
cat("when the starting point is 4.7, alpha is 0.25, the MLE for theta is" , result34)

alpha <- 0.25
theta0 <- 7
result35 <- fixedPoint(gprime, theta0, alpha)
cat("when the starting point is 7, alpha is 0.25, the MLE for theta is" , result35)

alpha <- 0.25
theta0 <- 8
result36 <- fixedPoint(gprime, theta0, alpha)
cat("when the starting point is 8, alpha is 0.25, the MLE for theta is" , result36)

alpha <- 0.25
theta0 <- 38
result37 <- fixedPoint(gprime, theta0, alpha)
cat("when the starting point is 38, alpha is 0.25, the MLE for theta is" , result37)
```

#### Problem 1(d) {-}

Apply fisher scoring to find the MLE
```{r q1e, echo = FALSE, eval=TRUE}
source('fisher_scoring_ iterations_XJ.R')
x <- c (1.77, -0.23, 2.76, 3.80, 3.47, 56.75, -1.34, 4.24, -2.44,
               3.29, 3.71, -2.40, 4.53, -0.07, -1.05, -13.87, -2.53, -1.75)
gprime <- function(theta) sum((-2) * (theta - x) / (1 + (theta-x)^2))
fisher <- length(x)/2

theta0 <- -11
result38 <- fisher_scoring(theta0, gprime, fisher)
cat("when the starting point is -11, the MLE for theta is" , result38)

theta0 <- -1
result39 <- fisher_scoring(theta0, gprime, fisher)
cat("when the starting point is -1, the MLE for theta is" , result39)

theta0 <- 0
result40 <- fisher_scoring(theta0, gprime, fisher)
cat("when the starting point is 0, the MLE for theta is" , result40)

theta0 <- 1.5
result41 <- fisher_scoring(theta0, gprime, fisher)
cat("when the starting point is -1.5, the MLE for theta is" , result41)

theta0 <- 4
result42 <- fisher_scoring(theta0, gprime, fisher)
cat("when the starting point is 4, the MLE for theta is" , result42)

theta0 <- 4.7
result43 <- fisher_scoring(theta0, gprime, fisher)
cat("when the starting point is 4.7, the MLE for theta is" , result43)

theta0 <- 7
result44 <- fisher_scoring(theta0, gprime, fisher)
cat("when the starting point is 7, the MLE for theta is" , result44)

theta0 <- 8
result45 <- fisher_scoring(theta0, gprime, fisher)
cat("when the starting point is 8, the MLE for theta is" , result45)

theta0 <- 38
result46 <- fisher_scoring(theta0, gprime, fisher)
cat("when the starting point is 38, the MLE for theta is" , result46)

```

#### Problem 1(e) {-}
Fisher scoring and Newton’s method both have the same asymptotic properties,
but for individual problems one may be computationally or analytically easier than the other. Generally, Fisher scoring works better in the beginning to make rapid improvements, while Newton’s method works better for refinement near the end.

Since many log likelihoods are approximately locally quadratic, scaled fixed-point iteration can be a very effective tool. The method is also generally quite stable and easy to code.

### Problem 2 {-}

#### Problem 2(a) {-}
log-likelihood  function: $l(\theta) = \ln({\sum_{i=1}^{n}\dfrac{1-cos(x-\theta)}{2\pi}}) = -n\ln{2\pi} + \sum_{i=1}^{n} \ln{[1-cos(x-\theta)]}$
The graph of the log-likelihood function of $\theta$ is plotted from $[-\pi,\pi]$
```{r q2a, echo = FALSE, eval=TRUE,fig.cap="\\label{fig.q2a} Log-likelihood function"}
# Plot the log-likelihood function
LogL <- function(theta,x) sum(-log(2 * pi) + log(1 - cos(x-theta)))
x_q2 <- c(3.91, 4.85, 2.28, 4.06, 3.70, 4.04, 5.46, 3.53, 2.28, 1.96, 
       2.53, 3.88, 2.22, 3.47, 4.82, 2.46, 2.99, 2.54, 0.52)
theta <- seq(-pi, pi, 0.0005)
plot (theta, sapply(theta, LogL, x_q2), type = "l", ylab = "LogL")
```

#### Problem 2(b) {-}

$\mathbb{E}[X | \theta] = \int_{0}^{2\pi} x p(x;\theta) dx = \dfrac{1}{2\pi}[\int_{0}^{2\pi}x dx - \int_{0}^{2\pi}xcos(x -\theta)] dx = \dfrac{1}{2\pi}[\dfrac{x^2}{2}\big|_{0}^{2\pi} - x sin(x - \theta)\big|_{0}^{2\pi} + \int_{0}^{2\pi} sin(x-\theta) dx] = \pi - sin\theta$

```{r 2b, echo = FALSE, eval=FALSE}
x_mean <- mean(x_q2)
cat("the sample mean is ", x_mean)
```

$\pi - sin\theta = 3.236842$

$\theta_0 = \hat{\theta}_{\text{moment}} = \arcsin(\pi - \bar{x})$

The method of moments of $\theta$ is -0.095394

#### Problem 2(c)(d) {-}
```{r q2c, echo = FALSE, eval=TRUE}
source('Newton-Raphson iterations_XJ.R')
x <- c(3.91, 4.85, 2.28, 4.06, 3.70, 4.04, 5.46, 3.53, 2.28, 1.96, 
       2.53, 3.88, 2.22, 3.47, 4.82, 2.46, 2.99, 2.54, 0.52)
gprime <- function(theta) sum(sin(x-theta)/(-1 + cos(x-theta)))
g2prime <- function(theta) sum(1/(cos(x - theta) - 1))

theta0 <- -0.095394
resultq20 <- Newton_Raphson(theta0, gprime, g2prime)
cat("when the starting point is -0.095394, the MLE for theta is" , resultq20)

theta0 <- -2.7
resultq21 <- Newton_Raphson(theta0, gprime, g2prime)
cat("when the starting point is -2.7, the MLE for theta is" , resultq21)

theta0 <- 2.7
resultq22 <- Newton_Raphson(theta0, gprime, g2prime)
cat("when the starting point is 2.7, the MLE for theta is" , resultq22)
```

#### Problem 2(e) {-}
```{r q2e, echo = FALSE, eval=TRUE}
source('Newton-Raphson iterations_XJ.R')
x <- c(3.91, 4.85, 2.28, 4.06, 3.70, 4.04, 5.46, 3.53, 2.28, 1.96, 
       2.53, 3.88, 2.22, 3.47, 4.82, 2.46, 2.99, 2.54, 0.52)
gprime <- function(theta) sum(sin(x-theta)/(-1 + cos(x-theta)))
g2prime <- function(theta) sum(1/(cos(x - theta) - 1))
theta0_group <- seq(-pi, pi, length.out=200)
outcome_group <- c()
for (theta0 in theta0_group) {outcome <- round(Newton_Raphson(theta0, gprime, g2prime), digits = 6)
outcome_group <- c(outcome_group, outcome)
}
outcome_unique <- unique(outcome_group)
print(outcome_unique)
```


### Problem 3 {-}

#### Problem 3(a) {-}

a) Fit the population growth model

According to the sample population, K is around 1100

the starting value of r is determined by $\ln(1024/2)/154 = 0.04$

```{r 3a, echo = TRUE, eval=TRUE}
beetles <- data.frame(
  days = c(0, 8, 28, 41, 63, 69, 97, 117, 135, 154),
  N_obv = c(2, 47, 192, 256, 768, 896, 1120, 896, 1184, 1024))
# set starting value
formula <- formula(N_obv~(K*N0)/(N0 + (K - N0) * exp(-r*days)))
K_start <- 1100
r_start <- 0.04
N0_start <- 2
# use nls model                
m <- nls(formula, data = beetles, start=list(K=K_start, r = r_start, N0 = N0_start))
coef(summary(m), start)
```

#### Problem 3(b) {-}

contour plot of the sum of squares errors

sum of squared errors: $\varepsilon = \sum_{i=1}^{n} (N_i-f(t_i;K,r))^2 = \sum_{i=1}^{n} (N_i-f(K,r))^2$

The contour plot of SSE
```{r q3b, echo = FALSE, eval=TRUE, fig.cap="\\label{fig.q3b} Contour Plot"}
SSE <- function(K, r) {return(sum(((N - K * 2)/(2 + (K - 2) * exp(-r * t)))^2))}
  t = c(0, 8, 28, 41, 63, 69, 97, 117, 135, 154)
  N = c(2, 47, 192, 256, 768, 896, 1120, 896, 1184, 1024)
z <- matrix(0, 100, 100, byrow = TRUE)
for (i in 1:100){
  for (j in 1:100){
    K <- 800 + 5 * j
    r <- 0 + .005 * i
    z[j,i] <- SSE(K, r)
  }
}
K <- seq(800, 1200, length.out = 100)
r <- seq(0, 0.7, length.out = 100)
contour(K, r, z, labels = NULL)
```






